{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow import flags\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cnn_tool as tool\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, sequence_length, num_classes, vocab_size,\n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.1)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/gpu:0'), tf.name_scope(\"embedding\"):\n",
    "#         with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)  # flip params                                                  \n",
    "#         self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits = self.scores) #  only named arguments accepted                                         \n",
    "#             losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = pd.read_csv(\"news_dataset.csv\", index_col = 0)\n",
    "contents = cnn[\"content\"]\n",
    "labels = cnn[\"label\"]\n",
    "max_document_length = 1000 # max_documemt_length : 2035\n",
    "\n",
    "# trandform document to vector\n",
    "x, vocabulary, vocab_size = tool.make_input(contents, max_document_length)\n",
    "y = tool.make_output(labels)\n",
    "x_train, x_test, y_train, y_test = tool.divide(x, y, train_prop = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of embedded vector (default: 128)\")\n",
    "flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
    "flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
      "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
      "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
      "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
      "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
      "Writing to /home/kookmin/work/U2017038/DL_class/dl_final_proj/text_cnn/runs/1529652859\n",
      "\n",
      "<tf.Variable 'global_step:0' shape=() dtype=int32_ref>\n",
      "2018-06-22T16:34:21.995663: step 1, loss 9.16293, acc 0.171875\n",
      "2018-06-22T16:34:22.230477: step 2, loss 8.6664, acc 0.21875\n",
      "2018-06-22T16:34:22.446324: step 3, loss 8.06985, acc 0.171875\n",
      "2018-06-22T16:34:22.654097: step 4, loss 6.5128, acc 0.21875\n",
      "2018-06-22T16:34:22.917062: step 5, loss 7.31061, acc 0.194444\n",
      "2018-06-22T16:34:23.142448: step 6, loss 5.01503, acc 0.40625\n",
      "2018-06-22T16:34:23.343956: step 7, loss 5.70609, acc 0.28125\n",
      "2018-06-22T16:34:23.567885: step 8, loss 6.7884, acc 0.296875\n",
      "2018-06-22T16:34:23.785238: step 9, loss 6.78541, acc 0.34375\n",
      "2018-06-22T16:34:23.960605: step 10, loss 4.63178, acc 0.416667\n",
      "2018-06-22T16:34:24.188887: step 11, loss 7.21356, acc 0.3125\n",
      "2018-06-22T16:34:24.383085: step 12, loss 6.56108, acc 0.34375\n",
      "2018-06-22T16:34:24.582026: step 13, loss 4.6699, acc 0.453125\n",
      "2018-06-22T16:34:24.797336: step 14, loss 4.26506, acc 0.5\n",
      "2018-06-22T16:34:24.971284: step 15, loss 4.89886, acc 0.416667\n",
      "2018-06-22T16:34:25.214929: step 16, loss 5.32348, acc 0.3125\n",
      "2018-06-22T16:34:25.413034: step 17, loss 5.13793, acc 0.359375\n",
      "2018-06-22T16:34:25.631351: step 18, loss 5.76916, acc 0.359375\n",
      "2018-06-22T16:34:25.850393: step 19, loss 4.76556, acc 0.40625\n",
      "2018-06-22T16:34:26.059251: step 20, loss 3.55944, acc 0.472222\n",
      "2018-06-22T16:34:26.228382: step 21, loss 4.42836, acc 0.453125\n",
      "2018-06-22T16:34:26.441384: step 22, loss 4.112, acc 0.546875\n",
      "2018-06-22T16:34:26.681773: step 23, loss 5.26063, acc 0.359375\n",
      "2018-06-22T16:34:26.917467: step 24, loss 5.37512, acc 0.359375\n",
      "2018-06-22T16:34:27.113060: step 25, loss 3.4428, acc 0.5\n",
      "2018-06-22T16:34:27.316466: step 26, loss 4.12745, acc 0.453125\n",
      "2018-06-22T16:34:27.529313: step 27, loss 3.25602, acc 0.53125\n",
      "2018-06-22T16:34:27.745261: step 28, loss 5.99321, acc 0.375\n",
      "2018-06-22T16:34:27.949186: step 29, loss 4.37584, acc 0.453125\n",
      "2018-06-22T16:34:28.130045: step 30, loss 5.41295, acc 0.361111\n",
      "2018-06-22T16:34:28.346280: step 31, loss 4.81574, acc 0.34375\n",
      "2018-06-22T16:34:28.554539: step 32, loss 5.17054, acc 0.40625\n",
      "2018-06-22T16:34:28.767033: step 33, loss 4.37889, acc 0.375\n",
      "2018-06-22T16:34:28.966779: step 34, loss 5.21253, acc 0.28125\n",
      "2018-06-22T16:34:29.141834: step 35, loss 3.17078, acc 0.388889\n",
      "2018-06-22T16:34:29.366318: step 36, loss 3.09609, acc 0.53125\n",
      "2018-06-22T16:34:29.569867: step 37, loss 3.70414, acc 0.453125\n",
      "2018-06-22T16:34:29.798259: step 38, loss 3.47321, acc 0.5625\n",
      "2018-06-22T16:34:29.998584: step 39, loss 3.4986, acc 0.484375\n",
      "2018-06-22T16:34:30.185511: step 40, loss 4.36072, acc 0.416667\n",
      "2018-06-22T16:34:30.405041: step 41, loss 4.03026, acc 0.484375\n",
      "2018-06-22T16:34:30.624748: step 42, loss 4.28806, acc 0.375\n",
      "2018-06-22T16:34:30.839415: step 43, loss 3.84216, acc 0.40625\n",
      "2018-06-22T16:34:31.077130: step 44, loss 3.82425, acc 0.4375\n",
      "2018-06-22T16:34:31.261054: step 45, loss 3.37592, acc 0.444444\n",
      "2018-06-22T16:34:31.492141: step 46, loss 3.40398, acc 0.5\n",
      "2018-06-22T16:34:31.696036: step 47, loss 3.16705, acc 0.53125\n",
      "2018-06-22T16:34:31.914233: step 48, loss 3.22371, acc 0.53125\n",
      "2018-06-22T16:34:32.148896: step 49, loss 2.92904, acc 0.59375\n",
      "2018-06-22T16:34:32.323470: step 50, loss 3.17764, acc 0.611111\n",
      "2018-06-22T16:34:32.536666: step 51, loss 2.55594, acc 0.5625\n",
      "2018-06-22T16:34:32.738182: step 52, loss 3.20775, acc 0.453125\n",
      "2018-06-22T16:34:32.971216: step 53, loss 2.29315, acc 0.625\n",
      "2018-06-22T16:34:33.169817: step 54, loss 2.47858, acc 0.578125\n",
      "2018-06-22T16:34:33.347327: step 55, loss 3.3437, acc 0.555556\n",
      "2018-06-22T16:34:33.555564: step 56, loss 3.5243, acc 0.4375\n",
      "2018-06-22T16:34:33.768118: step 57, loss 2.42811, acc 0.609375\n",
      "2018-06-22T16:34:33.983038: step 58, loss 2.22345, acc 0.609375\n",
      "2018-06-22T16:34:34.181613: step 59, loss 3.22495, acc 0.546875\n",
      "2018-06-22T16:34:34.376339: step 60, loss 2.80816, acc 0.638889\n",
      "2018-06-22T16:34:34.595171: step 61, loss 2.88454, acc 0.53125\n",
      "2018-06-22T16:34:34.822878: step 62, loss 2.3033, acc 0.671875\n",
      "2018-06-22T16:34:35.032336: step 63, loss 2.22343, acc 0.578125\n",
      "2018-06-22T16:34:35.247457: step 64, loss 2.81982, acc 0.546875\n",
      "2018-06-22T16:34:35.434828: step 65, loss 3.57578, acc 0.5\n",
      "2018-06-22T16:34:35.641167: step 66, loss 2.73916, acc 0.609375\n",
      "2018-06-22T16:34:35.867885: step 67, loss 2.18829, acc 0.65625\n",
      "2018-06-22T16:34:36.052373: step 68, loss 2.06011, acc 0.6875\n",
      "2018-06-22T16:34:36.289359: step 69, loss 2.13566, acc 0.609375\n",
      "2018-06-22T16:34:36.466865: step 70, loss 2.6472, acc 0.611111\n",
      "2018-06-22T16:34:36.656128: step 71, loss 2.50298, acc 0.5625\n",
      "2018-06-22T16:34:36.882581: step 72, loss 2.49674, acc 0.59375\n",
      "2018-06-22T16:34:37.085596: step 73, loss 1.88638, acc 0.59375\n",
      "2018-06-22T16:34:37.272031: step 74, loss 2.39074, acc 0.625\n",
      "2018-06-22T16:34:37.439854: step 75, loss 2.98578, acc 0.555556\n",
      "2018-06-22T16:34:37.647798: step 76, loss 2.17733, acc 0.59375\n",
      "2018-06-22T16:34:37.857572: step 77, loss 2.43068, acc 0.59375\n",
      "2018-06-22T16:34:38.060998: step 78, loss 1.76998, acc 0.6875\n",
      "2018-06-22T16:34:38.313641: step 79, loss 2.64921, acc 0.59375\n",
      "2018-06-22T16:34:38.480167: step 80, loss 2.0237, acc 0.75\n",
      "2018-06-22T16:34:38.700782: step 81, loss 2.5034, acc 0.6875\n",
      "2018-06-22T16:34:38.980415: step 82, loss 1.80201, acc 0.734375\n",
      "2018-06-22T16:34:39.225800: step 83, loss 2.05057, acc 0.703125\n",
      "2018-06-22T16:34:39.471460: step 84, loss 1.48556, acc 0.6875\n",
      "2018-06-22T16:34:39.658595: step 85, loss 2.57903, acc 0.583333\n",
      "2018-06-22T16:34:39.872622: step 86, loss 1.78541, acc 0.703125\n",
      "2018-06-22T16:34:40.078309: step 87, loss 2.15951, acc 0.609375\n",
      "2018-06-22T16:34:40.276776: step 88, loss 1.50395, acc 0.6875\n",
      "2018-06-22T16:34:40.460551: step 89, loss 2.03745, acc 0.734375\n",
      "2018-06-22T16:34:40.633647: step 90, loss 2.01826, acc 0.722222\n",
      "2018-06-22T16:34:40.844969: step 91, loss 1.88243, acc 0.671875\n",
      "2018-06-22T16:34:41.048185: step 92, loss 1.98837, acc 0.703125\n",
      "2018-06-22T16:34:41.264803: step 93, loss 2.02754, acc 0.59375\n",
      "2018-06-22T16:34:41.468495: step 94, loss 1.59952, acc 0.65625\n",
      "2018-06-22T16:34:41.660201: step 95, loss 2.2317, acc 0.611111\n",
      "2018-06-22T16:34:41.876050: step 96, loss 1.91464, acc 0.703125\n",
      "2018-06-22T16:34:42.098593: step 97, loss 2.28825, acc 0.625\n",
      "2018-06-22T16:34:42.304288: step 98, loss 1.71497, acc 0.734375\n",
      "2018-06-22T16:34:42.527061: step 99, loss 1.86495, acc 0.703125\n",
      "2018-06-22T16:34:42.683406: step 100, loss 1.47727, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-06-22T16:34:42.798987: step 100, loss 2.10607, acc 0.589041\n",
      "\n",
      "Saved model checkpoint to /home/kookmin/work/U2017038/DL_class/dl_final_proj/text_cnn/runs/1529652859/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-22T16:34:43.236053: step 101, loss 2.09526, acc 0.703125\n",
      "2018-06-22T16:34:43.457742: step 102, loss 1.43208, acc 0.734375\n",
      "2018-06-22T16:34:43.661221: step 103, loss 1.95638, acc 0.703125\n",
      "2018-06-22T16:34:43.883950: step 104, loss 1.42695, acc 0.734375\n",
      "2018-06-22T16:34:44.045326: step 105, loss 1.95079, acc 0.694444\n",
      "2018-06-22T16:34:44.251069: step 106, loss 1.90465, acc 0.671875\n",
      "2018-06-22T16:34:44.486617: step 107, loss 1.73467, acc 0.734375\n",
      "2018-06-22T16:34:44.688304: step 108, loss 1.52785, acc 0.734375\n",
      "2018-06-22T16:34:44.918206: step 109, loss 2.16141, acc 0.59375\n",
      "2018-06-22T16:34:45.114229: step 110, loss 1.0143, acc 0.833333\n",
      "2018-06-22T16:34:45.341105: step 111, loss 1.81772, acc 0.6875\n",
      "2018-06-22T16:34:45.576030: step 112, loss 1.28056, acc 0.765625\n",
      "2018-06-22T16:34:45.783566: step 113, loss 2.01795, acc 0.703125\n",
      "2018-06-22T16:34:45.982512: step 114, loss 1.46467, acc 0.75\n",
      "2018-06-22T16:34:46.151170: step 115, loss 1.21495, acc 0.805556\n",
      "2018-06-22T16:34:46.381958: step 116, loss 2.24288, acc 0.703125\n",
      "2018-06-22T16:34:46.614586: step 117, loss 1.29502, acc 0.765625\n",
      "2018-06-22T16:34:46.824901: step 118, loss 1.50362, acc 0.765625\n",
      "2018-06-22T16:34:47.044246: step 119, loss 1.63039, acc 0.71875\n",
      "2018-06-22T16:34:47.225174: step 120, loss 1.39681, acc 0.777778\n",
      "2018-06-22T16:34:47.398929: step 121, loss 1.32024, acc 0.75\n",
      "2018-06-22T16:34:47.591731: step 122, loss 1.17149, acc 0.84375\n",
      "2018-06-22T16:34:47.819842: step 123, loss 1.58305, acc 0.78125\n",
      "2018-06-22T16:34:48.043498: step 124, loss 1.50765, acc 0.703125\n",
      "2018-06-22T16:34:48.229916: step 125, loss 1.11965, acc 0.777778\n",
      "2018-06-22T16:34:48.458717: step 126, loss 1.2164, acc 0.84375\n",
      "2018-06-22T16:34:48.641790: step 127, loss 1.72125, acc 0.796875\n",
      "2018-06-22T16:34:48.847916: step 128, loss 1.01809, acc 0.828125\n",
      "2018-06-22T16:34:49.058030: step 129, loss 1.55356, acc 0.78125\n",
      "2018-06-22T16:34:49.249033: step 130, loss 1.84925, acc 0.666667\n",
      "2018-06-22T16:34:49.471721: step 131, loss 1.18823, acc 0.84375\n",
      "2018-06-22T16:34:49.692600: step 132, loss 1.73359, acc 0.703125\n",
      "2018-06-22T16:34:49.877149: step 133, loss 0.740149, acc 0.875\n",
      "2018-06-22T16:34:50.083155: step 134, loss 1.71772, acc 0.765625\n",
      "2018-06-22T16:34:50.240932: step 135, loss 1.1819, acc 0.777778\n",
      "2018-06-22T16:34:50.424313: step 136, loss 1.35761, acc 0.8125\n",
      "2018-06-22T16:34:50.603975: step 137, loss 1.08078, acc 0.84375\n",
      "2018-06-22T16:34:50.798097: step 138, loss 1.19303, acc 0.75\n",
      "2018-06-22T16:34:51.003231: step 139, loss 1.24888, acc 0.765625\n",
      "2018-06-22T16:34:51.176338: step 140, loss 1.14262, acc 0.833333\n",
      "2018-06-22T16:34:51.385160: step 141, loss 1.0742, acc 0.828125\n",
      "2018-06-22T16:34:51.583922: step 142, loss 1.57778, acc 0.765625\n",
      "2018-06-22T16:34:51.798886: step 143, loss 1.5005, acc 0.78125\n",
      "2018-06-22T16:34:51.971052: step 144, loss 1.12593, acc 0.734375\n",
      "2018-06-22T16:34:52.112184: step 145, loss 1.22813, acc 0.777778\n",
      "2018-06-22T16:34:52.305445: step 146, loss 0.925825, acc 0.8125\n",
      "2018-06-22T16:34:52.525581: step 147, loss 1.66405, acc 0.71875\n",
      "2018-06-22T16:34:52.720894: step 148, loss 1.30357, acc 0.84375\n",
      "2018-06-22T16:34:52.929010: step 149, loss 1.19975, acc 0.8125\n",
      "2018-06-22T16:34:53.124015: step 150, loss 0.826671, acc 0.861111\n",
      "2018-06-22T16:34:53.329747: step 151, loss 1.3709, acc 0.734375\n",
      "2018-06-22T16:34:53.535770: step 152, loss 0.985788, acc 0.8125\n",
      "2018-06-22T16:34:53.752985: step 153, loss 1.04093, acc 0.84375\n",
      "2018-06-22T16:34:53.964680: step 154, loss 1.44321, acc 0.75\n",
      "2018-06-22T16:34:54.138795: step 155, loss 0.841574, acc 0.888889\n",
      "2018-06-22T16:34:54.331925: step 156, loss 1.0152, acc 0.859375\n",
      "2018-06-22T16:34:54.542913: step 157, loss 1.40959, acc 0.765625\n",
      "2018-06-22T16:34:54.749337: step 158, loss 1.23436, acc 0.796875\n",
      "2018-06-22T16:34:54.960458: step 159, loss 1.26414, acc 0.828125\n",
      "2018-06-22T16:34:55.135701: step 160, loss 1.3157, acc 0.833333\n",
      "2018-06-22T16:34:55.354988: step 161, loss 1.48395, acc 0.75\n",
      "2018-06-22T16:34:55.570062: step 162, loss 0.857694, acc 0.875\n",
      "2018-06-22T16:34:55.783282: step 163, loss 1.38171, acc 0.765625\n",
      "2018-06-22T16:34:55.984293: step 164, loss 0.943116, acc 0.828125\n",
      "2018-06-22T16:34:56.179172: step 165, loss 1.02791, acc 0.777778\n",
      "2018-06-22T16:34:56.387396: step 166, loss 0.941725, acc 0.859375\n",
      "2018-06-22T16:34:56.576026: step 167, loss 1.0913, acc 0.84375\n",
      "2018-06-22T16:34:56.811311: step 168, loss 1.60446, acc 0.78125\n",
      "2018-06-22T16:34:57.031132: step 169, loss 1.05517, acc 0.84375\n",
      "2018-06-22T16:34:57.203325: step 170, loss 1.32143, acc 0.777778\n",
      "2018-06-22T16:34:57.415128: step 171, loss 1.12357, acc 0.796875\n",
      "2018-06-22T16:34:57.641817: step 172, loss 1.4653, acc 0.75\n",
      "2018-06-22T16:34:57.834922: step 173, loss 1.16163, acc 0.875\n",
      "2018-06-22T16:34:58.031563: step 174, loss 1.00832, acc 0.828125\n",
      "2018-06-22T16:34:58.210814: step 175, loss 0.973703, acc 0.805556\n",
      "2018-06-22T16:34:58.424530: step 176, loss 1.11975, acc 0.875\n",
      "2018-06-22T16:34:58.637931: step 177, loss 0.852968, acc 0.875\n",
      "2018-06-22T16:34:58.804502: step 178, loss 1.13379, acc 0.765625\n",
      "2018-06-22T16:34:59.018313: step 179, loss 1.08521, acc 0.84375\n",
      "2018-06-22T16:34:59.188806: step 180, loss 0.724609, acc 0.833333\n",
      "2018-06-22T16:34:59.415179: step 181, loss 1.3152, acc 0.796875\n",
      "2018-06-22T16:34:59.630404: step 182, loss 0.862108, acc 0.859375\n",
      "2018-06-22T16:34:59.840097: step 183, loss 0.908526, acc 0.8125\n",
      "2018-06-22T16:35:00.068260: step 184, loss 1.02277, acc 0.84375\n",
      "2018-06-22T16:35:00.248732: step 185, loss 1.19773, acc 0.777778\n",
      "2018-06-22T16:35:00.473966: step 186, loss 1.10558, acc 0.8125\n",
      "2018-06-22T16:35:00.664214: step 187, loss 1.24297, acc 0.8125\n",
      "2018-06-22T16:35:00.862216: step 188, loss 0.897238, acc 0.890625\n",
      "2018-06-22T16:35:01.072748: step 189, loss 1.22531, acc 0.828125\n",
      "2018-06-22T16:35:01.261914: step 190, loss 0.507432, acc 0.972222\n",
      "2018-06-22T16:35:01.472145: step 191, loss 1.45589, acc 0.8125\n",
      "2018-06-22T16:35:01.703086: step 192, loss 1.42194, acc 0.8125\n",
      "2018-06-22T16:35:01.907778: step 193, loss 1.05135, acc 0.8125\n",
      "2018-06-22T16:35:02.123577: step 194, loss 0.837149, acc 0.875\n",
      "2018-06-22T16:35:02.292451: step 195, loss 1.11283, acc 0.805556\n",
      "2018-06-22T16:35:02.489491: step 196, loss 0.858684, acc 0.8125\n",
      "2018-06-22T16:35:02.696423: step 197, loss 1.16821, acc 0.859375\n",
      "2018-06-22T16:35:02.907575: step 198, loss 1.50534, acc 0.78125\n",
      "2018-06-22T16:35:03.121399: step 199, loss 1.15598, acc 0.828125\n",
      "2018-06-22T16:35:03.306507: step 200, loss 1.53871, acc 0.777778\n",
      "\n",
      "Evaluation:\n",
      "2018-06-22T16:35:03.319909: step 200, loss 1.94488, acc 0.589041\n",
      "\n",
      "Saved model checkpoint to /home/kookmin/work/U2017038/DL_class/dl_final_proj/text_cnn/runs/1529652859/checkpoints/model-200\n",
      "\n",
      "2018-06-22T16:35:03.697600: step 201, loss 0.805652, acc 0.859375\n",
      "2018-06-22T16:35:03.943923: step 202, loss 0.992105, acc 0.84375\n",
      "2018-06-22T16:35:04.157112: step 203, loss 1.19767, acc 0.78125\n",
      "2018-06-22T16:35:04.368510: step 204, loss 0.68909, acc 0.875\n",
      "2018-06-22T16:35:04.571452: step 205, loss 0.959612, acc 0.888889\n",
      "2018-06-22T16:35:04.777012: step 206, loss 1.0787, acc 0.78125\n",
      "2018-06-22T16:35:04.989904: step 207, loss 0.842711, acc 0.921875\n",
      "2018-06-22T16:35:05.208377: step 208, loss 1.129, acc 0.78125\n",
      "2018-06-22T16:35:05.425158: step 209, loss 1.0213, acc 0.859375\n",
      "2018-06-22T16:35:05.609209: step 210, loss 0.603377, acc 0.888889\n",
      "2018-06-22T16:35:05.832846: step 211, loss 0.895898, acc 0.859375\n",
      "2018-06-22T16:35:06.041192: step 212, loss 0.846203, acc 0.859375\n",
      "2018-06-22T16:35:06.259905: step 213, loss 0.620071, acc 0.921875\n",
      "2018-06-22T16:35:06.460595: step 214, loss 0.928493, acc 0.828125\n",
      "2018-06-22T16:35:06.609520: step 215, loss 0.807385, acc 0.888889\n",
      "2018-06-22T16:35:06.817753: step 216, loss 0.82014, acc 0.859375\n",
      "2018-06-22T16:35:07.039573: step 217, loss 0.72533, acc 0.859375\n",
      "2018-06-22T16:35:07.236528: step 218, loss 1.05987, acc 0.796875\n",
      "2018-06-22T16:35:07.457428: step 219, loss 0.812116, acc 0.875\n",
      "2018-06-22T16:35:07.641541: step 220, loss 0.771931, acc 0.916667\n",
      "2018-06-22T16:35:07.866706: step 221, loss 0.939059, acc 0.828125\n",
      "2018-06-22T16:35:08.081352: step 222, loss 0.688168, acc 0.921875\n",
      "2018-06-22T16:35:08.302676: step 223, loss 0.866459, acc 0.8125\n",
      "2018-06-22T16:35:08.514886: step 224, loss 0.871379, acc 0.859375\n",
      "2018-06-22T16:35:08.700157: step 225, loss 1.06272, acc 0.777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-22T16:35:08.935581: step 226, loss 0.854334, acc 0.875\n",
      "2018-06-22T16:35:09.153269: step 227, loss 0.851761, acc 0.84375\n",
      "2018-06-22T16:35:09.391824: step 228, loss 0.996407, acc 0.78125\n",
      "2018-06-22T16:35:09.610835: step 229, loss 0.57113, acc 0.953125\n",
      "2018-06-22T16:35:09.802841: step 230, loss 0.60051, acc 0.916667\n",
      "2018-06-22T16:35:10.011539: step 231, loss 1.07028, acc 0.890625\n",
      "2018-06-22T16:35:10.220634: step 232, loss 0.756386, acc 0.890625\n",
      "2018-06-22T16:35:10.417558: step 233, loss 0.729369, acc 0.875\n",
      "2018-06-22T16:35:10.639235: step 234, loss 0.838809, acc 0.890625\n",
      "2018-06-22T16:35:10.819815: step 235, loss 0.972569, acc 0.805556\n",
      "2018-06-22T16:35:11.027370: step 236, loss 1.12562, acc 0.859375\n",
      "2018-06-22T16:35:11.244603: step 237, loss 1.0938, acc 0.8125\n",
      "2018-06-22T16:35:11.448612: step 238, loss 0.66319, acc 0.84375\n",
      "2018-06-22T16:35:11.650632: step 239, loss 0.628021, acc 0.890625\n",
      "2018-06-22T16:35:11.831353: step 240, loss 1.08617, acc 0.833333\n",
      "2018-06-22T16:35:12.024509: step 241, loss 1.0524, acc 0.859375\n",
      "2018-06-22T16:35:12.235333: step 242, loss 0.769919, acc 0.890625\n",
      "2018-06-22T16:35:12.451801: step 243, loss 0.81256, acc 0.890625\n",
      "2018-06-22T16:35:12.678770: step 244, loss 0.715449, acc 0.90625\n",
      "2018-06-22T16:35:12.875858: step 245, loss 0.673422, acc 0.916667\n",
      "2018-06-22T16:35:13.096794: step 246, loss 1.05592, acc 0.875\n",
      "2018-06-22T16:35:13.322200: step 247, loss 0.849135, acc 0.890625\n",
      "2018-06-22T16:35:13.544079: step 248, loss 0.731341, acc 0.875\n",
      "2018-06-22T16:35:13.749101: step 249, loss 0.757778, acc 0.859375\n",
      "2018-06-22T16:35:13.944458: step 250, loss 0.802004, acc 0.888889\n",
      "2018-06-22T16:35:14.153807: step 251, loss 0.70466, acc 0.890625\n",
      "2018-06-22T16:35:14.350348: step 252, loss 0.920188, acc 0.84375\n",
      "2018-06-22T16:35:14.571272: step 253, loss 0.890783, acc 0.859375\n",
      "2018-06-22T16:35:14.766064: step 254, loss 0.810511, acc 0.90625\n",
      "2018-06-22T16:35:14.952509: step 255, loss 1.02669, acc 0.805556\n",
      "2018-06-22T16:35:15.166480: step 256, loss 0.748735, acc 0.890625\n",
      "2018-06-22T16:35:15.357219: step 257, loss 0.75907, acc 0.890625\n",
      "2018-06-22T16:35:15.573904: step 258, loss 0.604278, acc 0.921875\n",
      "2018-06-22T16:35:15.762763: step 259, loss 0.60663, acc 0.9375\n",
      "2018-06-22T16:35:15.910885: step 260, loss 1.03106, acc 0.833333\n",
      "2018-06-22T16:35:16.129526: step 261, loss 0.738293, acc 0.84375\n",
      "2018-06-22T16:35:16.325415: step 262, loss 0.812264, acc 0.8125\n",
      "2018-06-22T16:35:16.549845: step 263, loss 0.569725, acc 0.921875\n",
      "2018-06-22T16:35:16.727293: step 264, loss 0.687554, acc 0.875\n",
      "2018-06-22T16:35:16.895950: step 265, loss 0.933224, acc 0.888889\n",
      "2018-06-22T16:35:17.119904: step 266, loss 0.670484, acc 0.875\n",
      "2018-06-22T16:35:17.308079: step 267, loss 0.715756, acc 0.84375\n",
      "2018-06-22T16:35:17.513125: step 268, loss 0.562148, acc 0.921875\n",
      "2018-06-22T16:35:17.697734: step 269, loss 0.484606, acc 0.953125\n",
      "2018-06-22T16:35:17.865248: step 270, loss 0.567887, acc 0.861111\n",
      "2018-06-22T16:35:18.081694: step 271, loss 0.629945, acc 0.921875\n",
      "2018-06-22T16:35:18.287158: step 272, loss 0.734948, acc 0.9375\n",
      "2018-06-22T16:35:18.504054: step 273, loss 0.753708, acc 0.859375\n",
      "2018-06-22T16:35:18.682624: step 274, loss 0.797284, acc 0.875\n",
      "2018-06-22T16:35:18.859494: step 275, loss 0.878242, acc 0.861111\n",
      "2018-06-22T16:35:19.052794: step 276, loss 1.09614, acc 0.8125\n",
      "2018-06-22T16:35:19.288090: step 277, loss 0.82234, acc 0.875\n",
      "2018-06-22T16:35:19.496654: step 278, loss 0.686963, acc 0.875\n",
      "2018-06-22T16:35:19.720799: step 279, loss 0.803283, acc 0.875\n",
      "2018-06-22T16:35:19.899244: step 280, loss 0.823333, acc 0.861111\n",
      "2018-06-22T16:35:20.088698: step 281, loss 0.651693, acc 0.875\n",
      "2018-06-22T16:35:20.284651: step 282, loss 0.669485, acc 0.90625\n",
      "2018-06-22T16:35:20.472991: step 283, loss 0.584305, acc 0.90625\n",
      "2018-06-22T16:35:20.697845: step 284, loss 0.732499, acc 0.921875\n",
      "2018-06-22T16:35:20.875003: step 285, loss 0.629363, acc 0.916667\n",
      "2018-06-22T16:35:21.082525: step 286, loss 0.811598, acc 0.84375\n",
      "2018-06-22T16:35:21.296671: step 287, loss 0.764235, acc 0.90625\n",
      "2018-06-22T16:35:21.486192: step 288, loss 0.727912, acc 0.875\n",
      "2018-06-22T16:35:21.666578: step 289, loss 0.535613, acc 0.9375\n",
      "2018-06-22T16:35:21.819816: step 290, loss 0.617178, acc 0.944444\n",
      "2018-06-22T16:35:22.020527: step 291, loss 0.891123, acc 0.875\n",
      "2018-06-22T16:35:22.238119: step 292, loss 0.631619, acc 0.921875\n",
      "2018-06-22T16:35:22.461242: step 293, loss 0.693825, acc 0.921875\n",
      "2018-06-22T16:35:22.662651: step 294, loss 0.937241, acc 0.890625\n",
      "2018-06-22T16:35:22.849538: step 295, loss 0.374134, acc 1\n",
      "2018-06-22T16:35:23.063894: step 296, loss 0.606332, acc 0.9375\n",
      "2018-06-22T16:35:23.268586: step 297, loss 0.595372, acc 0.90625\n",
      "2018-06-22T16:35:23.460979: step 298, loss 0.709603, acc 0.859375\n",
      "2018-06-22T16:35:23.683340: step 299, loss 0.809889, acc 0.875\n",
      "2018-06-22T16:35:23.826513: step 300, loss 0.737741, acc 0.888889\n",
      "\n",
      "Evaluation:\n",
      "2018-06-22T16:35:23.838389: step 300, loss 2.42472, acc 0.575342\n",
      "\n",
      "Saved model checkpoint to /home/kookmin/work/U2017038/DL_class/dl_final_proj/text_cnn/runs/1529652859/checkpoints/model-300\n",
      "\n",
      "2018-06-22T16:35:24.183006: step 301, loss 0.661674, acc 0.90625\n",
      "2018-06-22T16:35:24.411021: step 302, loss 0.604537, acc 0.890625\n",
      "2018-06-22T16:35:24.627766: step 303, loss 1.08746, acc 0.84375\n",
      "2018-06-22T16:35:24.830347: step 304, loss 0.604613, acc 0.953125\n",
      "2018-06-22T16:35:25.010656: step 305, loss 0.641574, acc 0.861111\n",
      "2018-06-22T16:35:25.192953: step 306, loss 0.735293, acc 0.890625\n",
      "2018-06-22T16:35:25.383483: step 307, loss 0.63331, acc 0.9375\n",
      "2018-06-22T16:35:25.588481: step 308, loss 0.534684, acc 0.90625\n",
      "2018-06-22T16:35:25.802919: step 309, loss 0.647013, acc 0.9375\n",
      "2018-06-22T16:35:25.992144: step 310, loss 0.477182, acc 0.972222\n",
      "2018-06-22T16:35:26.197744: step 311, loss 0.701514, acc 0.890625\n",
      "2018-06-22T16:35:26.425678: step 312, loss 0.569428, acc 0.90625\n",
      "2018-06-22T16:35:26.623963: step 313, loss 0.701219, acc 0.90625\n",
      "2018-06-22T16:35:26.806349: step 314, loss 0.734821, acc 0.921875\n",
      "2018-06-22T16:35:27.000770: step 315, loss 0.725104, acc 0.888889\n",
      "2018-06-22T16:35:27.207106: step 316, loss 0.71543, acc 0.96875\n",
      "2018-06-22T16:35:27.421053: step 317, loss 0.760684, acc 0.84375\n",
      "2018-06-22T16:35:27.618945: step 318, loss 0.884377, acc 0.875\n",
      "2018-06-22T16:35:27.798250: step 319, loss 0.477577, acc 0.953125\n",
      "2018-06-22T16:35:27.967783: step 320, loss 0.81117, acc 0.916667\n",
      "2018-06-22T16:35:28.152504: step 321, loss 0.619156, acc 0.875\n",
      "2018-06-22T16:35:28.364426: step 322, loss 0.521298, acc 0.90625\n",
      "2018-06-22T16:35:28.577472: step 323, loss 0.614203, acc 0.9375\n",
      "2018-06-22T16:35:28.772652: step 324, loss 0.812116, acc 0.875\n",
      "2018-06-22T16:35:28.932590: step 325, loss 0.549755, acc 0.944444\n",
      "2018-06-22T16:35:29.091511: step 326, loss 0.517443, acc 0.921875\n",
      "2018-06-22T16:35:29.288844: step 327, loss 0.473696, acc 0.96875\n",
      "2018-06-22T16:35:29.512149: step 328, loss 0.739263, acc 0.90625\n",
      "2018-06-22T16:35:29.747827: step 329, loss 0.771298, acc 0.90625\n",
      "2018-06-22T16:35:29.915162: step 330, loss 1.07379, acc 0.75\n",
      "2018-06-22T16:35:30.119835: step 331, loss 0.916676, acc 0.84375\n",
      "2018-06-22T16:35:30.300611: step 332, loss 0.720508, acc 0.90625\n",
      "2018-06-22T16:35:30.479148: step 333, loss 0.549451, acc 0.9375\n",
      "2018-06-22T16:35:30.677130: step 334, loss 0.720562, acc 0.875\n",
      "2018-06-22T16:35:30.852145: step 335, loss 1.19829, acc 0.861111\n",
      "2018-06-22T16:35:31.062533: step 336, loss 0.806522, acc 0.84375\n",
      "2018-06-22T16:35:31.252583: step 337, loss 0.70916, acc 0.875\n",
      "2018-06-22T16:35:31.459451: step 338, loss 0.566058, acc 0.9375\n",
      "2018-06-22T16:35:31.657855: step 339, loss 0.547788, acc 0.90625\n",
      "2018-06-22T16:35:31.853905: step 340, loss 0.635174, acc 0.916667\n",
      "2018-06-22T16:35:32.053410: step 341, loss 0.663326, acc 0.890625\n",
      "2018-06-22T16:35:32.262674: step 342, loss 0.493439, acc 0.921875\n",
      "2018-06-22T16:35:32.464738: step 343, loss 0.65679, acc 0.90625\n",
      "2018-06-22T16:35:32.637606: step 344, loss 0.633578, acc 0.90625\n",
      "2018-06-22T16:35:32.804538: step 345, loss 0.405707, acc 0.972222\n",
      "2018-06-22T16:35:33.006799: step 346, loss 0.580522, acc 0.90625\n",
      "2018-06-22T16:35:33.219239: step 347, loss 0.732104, acc 0.890625\n",
      "2018-06-22T16:35:33.439699: step 348, loss 0.813385, acc 0.859375\n",
      "2018-06-22T16:35:33.669409: step 349, loss 0.729772, acc 0.9375\n",
      "2018-06-22T16:35:33.846377: step 350, loss 0.646608, acc 0.944444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-22T16:35:34.033038: step 351, loss 0.732546, acc 0.890625\n",
      "2018-06-22T16:35:34.261395: step 352, loss 0.492554, acc 0.9375\n",
      "2018-06-22T16:35:34.485103: step 353, loss 0.56962, acc 0.921875\n",
      "2018-06-22T16:35:34.677718: step 354, loss 1.10195, acc 0.875\n",
      "2018-06-22T16:35:34.850293: step 355, loss 0.491303, acc 0.944444\n",
      "2018-06-22T16:35:35.029206: step 356, loss 0.464487, acc 0.921875\n",
      "2018-06-22T16:35:35.223391: step 357, loss 0.616214, acc 0.9375\n",
      "2018-06-22T16:35:35.423760: step 358, loss 0.668448, acc 0.9375\n",
      "2018-06-22T16:35:35.632458: step 359, loss 0.5592, acc 0.90625\n",
      "2018-06-22T16:35:35.817449: step 360, loss 0.70665, acc 0.861111\n",
      "2018-06-22T16:35:36.016816: step 361, loss 0.47868, acc 0.9375\n",
      "2018-06-22T16:35:36.225113: step 362, loss 0.496205, acc 0.9375\n",
      "2018-06-22T16:35:36.421900: step 363, loss 0.480303, acc 0.9375\n",
      "2018-06-22T16:35:36.628605: step 364, loss 0.473332, acc 0.9375\n",
      "2018-06-22T16:35:36.808958: step 365, loss 0.644039, acc 0.916667\n",
      "2018-06-22T16:35:37.017664: step 366, loss 0.480287, acc 0.921875\n",
      "2018-06-22T16:35:37.200503: step 367, loss 0.549661, acc 0.890625\n",
      "2018-06-22T16:35:37.410764: step 368, loss 0.382535, acc 0.96875\n",
      "2018-06-22T16:35:37.613306: step 369, loss 0.566844, acc 0.9375\n",
      "2018-06-22T16:35:37.793165: step 370, loss 0.436545, acc 0.916667\n",
      "2018-06-22T16:35:37.990309: step 371, loss 0.401408, acc 0.96875\n",
      "2018-06-22T16:35:38.171739: step 372, loss 0.501992, acc 0.953125\n",
      "2018-06-22T16:35:38.365355: step 373, loss 0.726541, acc 0.953125\n",
      "2018-06-22T16:35:38.582732: step 374, loss 0.522023, acc 0.90625\n",
      "2018-06-22T16:35:38.768603: step 375, loss 0.552506, acc 0.888889\n",
      "2018-06-22T16:35:38.978625: step 376, loss 0.504867, acc 0.953125\n",
      "2018-06-22T16:35:39.186001: step 377, loss 0.537351, acc 0.9375\n",
      "2018-06-22T16:35:39.403525: step 378, loss 0.435306, acc 0.953125\n",
      "2018-06-22T16:35:39.606552: step 379, loss 0.46526, acc 0.953125\n",
      "2018-06-22T16:35:39.789906: step 380, loss 0.465617, acc 0.944444\n",
      "2018-06-22T16:35:39.969401: step 381, loss 0.516417, acc 0.90625\n",
      "2018-06-22T16:35:40.175839: step 382, loss 0.678968, acc 0.890625\n",
      "2018-06-22T16:35:40.402096: step 383, loss 0.481751, acc 0.953125\n",
      "2018-06-22T16:35:40.605721: step 384, loss 0.617956, acc 0.9375\n",
      "2018-06-22T16:35:40.790867: step 385, loss 0.550577, acc 0.916667\n",
      "2018-06-22T16:35:40.990425: step 386, loss 0.603103, acc 0.90625\n",
      "2018-06-22T16:35:41.231783: step 387, loss 0.339884, acc 1\n",
      "2018-06-22T16:35:41.456798: step 388, loss 0.580959, acc 0.90625\n",
      "2018-06-22T16:35:41.676778: step 389, loss 0.56416, acc 0.90625\n",
      "2018-06-22T16:35:41.859831: step 390, loss 0.503891, acc 0.916667\n",
      "2018-06-22T16:35:42.069029: step 391, loss 0.470505, acc 0.9375\n",
      "2018-06-22T16:35:42.279648: step 392, loss 0.464511, acc 0.921875\n",
      "2018-06-22T16:35:42.507579: step 393, loss 0.494205, acc 0.921875\n",
      "2018-06-22T16:35:42.723485: step 394, loss 0.493598, acc 0.96875\n",
      "2018-06-22T16:35:42.905967: step 395, loss 0.835778, acc 0.861111\n",
      "2018-06-22T16:35:43.110371: step 396, loss 0.64326, acc 0.921875\n",
      "2018-06-22T16:35:43.349092: step 397, loss 0.451201, acc 0.953125\n",
      "2018-06-22T16:35:43.556994: step 398, loss 0.473417, acc 0.921875\n",
      "2018-06-22T16:35:43.771877: step 399, loss 0.523373, acc 0.890625\n",
      "2018-06-22T16:35:43.974008: step 400, loss 0.378882, acc 0.972222\n",
      "\n",
      "Evaluation:\n",
      "2018-06-22T16:35:43.985673: step 400, loss 1.90019, acc 0.589041\n",
      "\n",
      "Saved model checkpoint to /home/kookmin/work/U2017038/DL_class/dl_final_proj/text_cnn/runs/1529652859/checkpoints/model-400\n",
      "\n",
      "2018-06-22T16:35:44.377215: step 401, loss 0.434779, acc 0.96875\n",
      "2018-06-22T16:35:44.589089: step 402, loss 0.44658, acc 0.9375\n",
      "2018-06-22T16:35:44.795187: step 403, loss 0.451417, acc 0.921875\n",
      "2018-06-22T16:35:44.996293: step 404, loss 0.462817, acc 0.921875\n",
      "2018-06-22T16:35:45.191632: step 405, loss 0.668613, acc 0.916667\n",
      "2018-06-22T16:35:45.394451: step 406, loss 0.585312, acc 0.921875\n",
      "2018-06-22T16:35:45.607950: step 407, loss 0.672272, acc 0.9375\n",
      "2018-06-22T16:35:45.850926: step 408, loss 0.800991, acc 0.859375\n",
      "2018-06-22T16:35:46.078377: step 409, loss 0.415792, acc 0.96875\n",
      "2018-06-22T16:35:46.275198: step 410, loss 0.633968, acc 0.916667\n",
      "2018-06-22T16:35:46.472587: step 411, loss 0.502791, acc 0.96875\n",
      "2018-06-22T16:35:46.670758: step 412, loss 0.395384, acc 0.984375\n",
      "2018-06-22T16:35:46.889237: step 413, loss 0.451418, acc 0.96875\n",
      "2018-06-22T16:35:47.121628: step 414, loss 0.525661, acc 0.90625\n",
      "2018-06-22T16:35:47.329987: step 415, loss 0.772863, acc 0.833333\n",
      "2018-06-22T16:35:47.520097: step 416, loss 0.566805, acc 0.90625\n",
      "2018-06-22T16:35:47.749806: step 417, loss 0.330735, acc 1\n",
      "2018-06-22T16:35:47.958815: step 418, loss 0.588583, acc 0.90625\n",
      "2018-06-22T16:35:48.169191: step 419, loss 0.655435, acc 0.953125\n",
      "2018-06-22T16:35:48.370408: step 420, loss 0.524831, acc 0.888889\n",
      "2018-06-22T16:35:48.589208: step 421, loss 0.418707, acc 0.953125\n",
      "2018-06-22T16:35:48.824173: step 422, loss 0.476872, acc 0.984375\n",
      "2018-06-22T16:35:49.022308: step 423, loss 0.566468, acc 0.921875\n",
      "2018-06-22T16:35:49.226678: step 424, loss 0.439766, acc 0.96875\n",
      "2018-06-22T16:35:49.407076: step 425, loss 0.545086, acc 0.916667\n",
      "2018-06-22T16:35:49.637208: step 426, loss 0.565354, acc 0.96875\n",
      "2018-06-22T16:35:49.855717: step 427, loss 0.48735, acc 0.953125\n",
      "2018-06-22T16:35:50.066909: step 428, loss 0.362244, acc 0.96875\n",
      "2018-06-22T16:35:50.267225: step 429, loss 0.759895, acc 0.875\n",
      "2018-06-22T16:35:50.455068: step 430, loss 0.48017, acc 0.944444\n",
      "2018-06-22T16:35:50.675201: step 431, loss 0.386672, acc 0.953125\n",
      "2018-06-22T16:35:50.900047: step 432, loss 0.435588, acc 0.953125\n",
      "2018-06-22T16:35:51.123132: step 433, loss 0.403601, acc 0.953125\n",
      "2018-06-22T16:35:51.338282: step 434, loss 0.494361, acc 0.921875\n",
      "2018-06-22T16:35:51.482604: step 435, loss 0.468233, acc 0.944444\n",
      "2018-06-22T16:35:51.700486: step 436, loss 0.849697, acc 0.84375\n",
      "2018-06-22T16:35:51.892804: step 437, loss 0.594065, acc 0.953125\n",
      "2018-06-22T16:35:52.116080: step 438, loss 0.692418, acc 0.890625\n",
      "2018-06-22T16:35:52.334076: step 439, loss 0.477011, acc 0.9375\n",
      "2018-06-22T16:35:52.511089: step 440, loss 0.480633, acc 0.888889\n",
      "2018-06-22T16:35:52.741507: step 441, loss 0.449035, acc 0.953125\n",
      "2018-06-22T16:35:52.944442: step 442, loss 0.408117, acc 0.9375\n",
      "2018-06-22T16:35:53.159096: step 443, loss 0.333352, acc 0.984375\n",
      "2018-06-22T16:35:53.377647: step 444, loss 0.535818, acc 0.96875\n",
      "2018-06-22T16:35:53.538779: step 445, loss 0.477931, acc 0.861111\n",
      "2018-06-22T16:35:53.760430: step 446, loss 0.667207, acc 0.90625\n",
      "2018-06-22T16:35:53.962498: step 447, loss 0.505548, acc 0.9375\n",
      "2018-06-22T16:35:54.170767: step 448, loss 0.429529, acc 0.9375\n",
      "2018-06-22T16:35:54.388544: step 449, loss 0.413702, acc 0.953125\n",
      "2018-06-22T16:35:54.572494: step 450, loss 0.335919, acc 0.972222\n",
      "2018-06-22T16:35:54.783504: step 451, loss 0.444511, acc 0.96875\n",
      "2018-06-22T16:35:54.995931: step 452, loss 0.652597, acc 0.921875\n",
      "2018-06-22T16:35:55.215212: step 453, loss 0.533761, acc 0.921875\n",
      "2018-06-22T16:35:55.454731: step 454, loss 0.359033, acc 0.96875\n",
      "2018-06-22T16:35:55.642400: step 455, loss 0.621237, acc 0.888889\n",
      "2018-06-22T16:35:55.846237: step 456, loss 0.478845, acc 0.9375\n",
      "2018-06-22T16:35:56.063498: step 457, loss 0.426847, acc 0.90625\n",
      "2018-06-22T16:35:56.288264: step 458, loss 0.319415, acc 0.984375\n",
      "2018-06-22T16:35:56.500913: step 459, loss 0.479288, acc 0.9375\n",
      "2018-06-22T16:35:56.706084: step 460, loss 0.460685, acc 0.944444\n",
      "2018-06-22T16:35:56.912616: step 461, loss 0.448805, acc 0.921875\n",
      "2018-06-22T16:35:57.143528: step 462, loss 0.599563, acc 0.9375\n",
      "2018-06-22T16:35:57.355199: step 463, loss 0.400925, acc 0.9375\n",
      "2018-06-22T16:35:57.567006: step 464, loss 0.490439, acc 0.90625\n",
      "2018-06-22T16:35:57.755514: step 465, loss 0.399087, acc 0.944444\n",
      "2018-06-22T16:35:57.970742: step 466, loss 0.639821, acc 0.90625\n",
      "2018-06-22T16:35:58.170937: step 467, loss 0.361448, acc 0.984375\n",
      "2018-06-22T16:35:58.390372: step 468, loss 0.422913, acc 0.9375\n",
      "2018-06-22T16:35:58.633514: step 469, loss 0.345425, acc 0.984375\n",
      "2018-06-22T16:35:58.795323: step 470, loss 0.506228, acc 0.944444\n",
      "2018-06-22T16:35:59.004089: step 471, loss 0.546895, acc 0.90625\n",
      "2018-06-22T16:35:59.218049: step 472, loss 0.667367, acc 0.890625\n",
      "2018-06-22T16:35:59.429284: step 473, loss 0.41748, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-22T16:35:59.640820: step 474, loss 0.598835, acc 0.875\n",
      "2018-06-22T16:35:59.835397: step 475, loss 0.496668, acc 0.944444\n",
      "2018-06-22T16:36:00.060749: step 476, loss 0.381641, acc 0.953125\n",
      "2018-06-22T16:36:00.247351: step 477, loss 0.357865, acc 0.984375\n",
      "2018-06-22T16:36:00.456610: step 478, loss 0.437296, acc 0.953125\n",
      "2018-06-22T16:36:00.680465: step 479, loss 0.503902, acc 0.921875\n",
      "2018-06-22T16:36:00.846186: step 480, loss 0.379788, acc 0.944444\n",
      "2018-06-22T16:36:01.039185: step 481, loss 0.325049, acc 0.984375\n",
      "2018-06-22T16:36:01.239352: step 482, loss 0.377515, acc 0.984375\n",
      "2018-06-22T16:36:01.474788: step 483, loss 0.304569, acc 1\n",
      "2018-06-22T16:36:01.679675: step 484, loss 0.389296, acc 0.96875\n",
      "2018-06-22T16:36:01.862650: step 485, loss 0.517708, acc 0.944444\n",
      "2018-06-22T16:36:02.048119: step 486, loss 0.387618, acc 0.953125\n",
      "2018-06-22T16:36:02.227969: step 487, loss 0.337502, acc 0.984375\n",
      "2018-06-22T16:36:02.459904: step 488, loss 0.357688, acc 0.96875\n",
      "2018-06-22T16:36:02.656267: step 489, loss 0.45456, acc 0.921875\n",
      "2018-06-22T16:36:02.850464: step 490, loss 0.553146, acc 0.888889\n",
      "2018-06-22T16:36:03.081756: step 491, loss 0.496217, acc 0.921875\n",
      "2018-06-22T16:36:03.299446: step 492, loss 0.376839, acc 0.953125\n",
      "2018-06-22T16:36:03.528990: step 493, loss 0.529613, acc 0.9375\n",
      "2018-06-22T16:36:03.729006: step 494, loss 0.301554, acc 1\n",
      "2018-06-22T16:36:03.904481: step 495, loss 0.529331, acc 0.916667\n",
      "2018-06-22T16:36:04.112021: step 496, loss 0.467451, acc 0.96875\n",
      "2018-06-22T16:36:04.326321: step 497, loss 0.444726, acc 0.96875\n",
      "2018-06-22T16:36:04.545179: step 498, loss 0.383313, acc 0.96875\n",
      "2018-06-22T16:36:04.758926: step 499, loss 0.366383, acc 0.984375\n",
      "2018-06-22T16:36:04.930590: step 500, loss 0.454648, acc 0.972222\n",
      "\n",
      "Evaluation:\n",
      "2018-06-22T16:36:04.955205: step 500, loss 1.38246, acc 0.630137\n",
      "\n",
      "Saved model checkpoint to /home/kookmin/work/U2017038/DL_class/dl_final_proj/text_cnn/runs/1529652859/checkpoints/model-500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. train the model and test\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1],\n",
    "                      num_classes=y_train.shape[1],\n",
    "                      vocab_size=vocab_size,\n",
    "                      embedding_size=FLAGS.embedding_dim,\n",
    "                      filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                      num_filters=FLAGS.num_filters,\n",
    "                      l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "            \"\"\"\n",
    "            Generates a batch iterator for a dataset.\n",
    "            \"\"\"\n",
    "            data = np.array(data)\n",
    "            data_size = len(data)\n",
    "            num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "            for epoch in range(num_epochs):\n",
    "                # Shuffle the data at each epoch\n",
    "                if shuffle:\n",
    "                    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                    shuffled_data = data[shuffle_indices]\n",
    "                else:\n",
    "                    shuffled_data = data\n",
    "                for batch_num in range(num_batches_per_epoch):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                    yield shuffled_data[start_index:end_index]\n",
    "\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "        # Training loop. For each batch...\n",
    "        print(global_step)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  text_cnn classification   train dataset  accuracy  1   ,\n",
    "test data  val_acc 0.63       ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
