{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv(\"news_dataset.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170101</td>\n",
       "      <td>朴대통령 \"뇌물죄, 완전히 엮은 것…세월호 허위 걷혀야\"(종합)</td>\n",
       "      <td>새해 첫날 청와대서 사실상 기자간담회…직무정지 23일 만에 첫 입장표명\"공모나 누구...</td>\n",
       "      <td>BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170102</td>\n",
       "      <td>정유라, 덴마크서 불법 체류 혐의로 체포···특검 “송환 협조중” (종합)</td>\n",
       "      <td>[아시아경제 정준영 기자] 이화여대 학사부정 및 삼성 특혜지원 의혹의 수혜자 겸 공...</td>\n",
       "      <td>BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170103</td>\n",
       "      <td>[단독]정유라, “(주사 아줌마)누구인지 알 것 같다”…현지 답변태도 분석, 사전 ...</td>\n",
       "      <td>덴마크 올보르 법원에서 잠시 휴정중 기자들의 질문에 답변하는 정유라씨  사진=현지교...</td>\n",
       "      <td>BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170104</td>\n",
       "      <td>[단독]\"정유라, 이대학장실 등 교내서 교수 6명에 학점취득 코치받아\"</td>\n",
       "      <td>[연합뉴스TV제공]김병욱, 교육부 자료 확인…\"학점 좋은이유 모른다더니\"담당교수들 ...</td>\n",
       "      <td>Politic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170105</td>\n",
       "      <td>윤전추 \"기억안나. 몰라. 말못해\"… 헌재 \"본인범죄 외 답해라\"</td>\n",
       "      <td>\"외부인 동행 없다\" 주장하다 \"세월호 당일 미용사 태워왔다\" 윤전추 헌재 탄핵심리...</td>\n",
       "      <td>BH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                              title  \\\n",
       "0  20170101                朴대통령 \"뇌물죄, 완전히 엮은 것…세월호 허위 걷혀야\"(종합)   \n",
       "1  20170102          정유라, 덴마크서 불법 체류 혐의로 체포···특검 “송환 협조중” (종합)   \n",
       "2  20170103  [단독]정유라, “(주사 아줌마)누구인지 알 것 같다”…현지 답변태도 분석, 사전 ...   \n",
       "3  20170104            [단독]\"정유라, 이대학장실 등 교내서 교수 6명에 학점취득 코치받아\"   \n",
       "4  20170105               윤전추 \"기억안나. 몰라. 말못해\"… 헌재 \"본인범죄 외 답해라\"   \n",
       "\n",
       "                                             content    label  \n",
       "0  새해 첫날 청와대서 사실상 기자간담회…직무정지 23일 만에 첫 입장표명\"공모나 누구...       BH  \n",
       "1  [아시아경제 정준영 기자] 이화여대 학사부정 및 삼성 특혜지원 의혹의 수혜자 겸 공...       BH  \n",
       "2  덴마크 올보르 법원에서 잠시 휴정중 기자들의 질문에 답변하는 정유라씨  사진=현지교...       BH  \n",
       "3  [연합뉴스TV제공]김병욱, 교육부 자료 확인…\"학점 좋은이유 모른다더니\"담당교수들 ...  Politic  \n",
       "4  \"외부인 동행 없다\" 주장하다 \"세월호 당일 미용사 태워왔다\" 윤전추 헌재 탄핵심리...       BH  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Admin', 'BH', 'Con/Party', 'Defence/Diplo', 'North', 'Politic'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 총 6개의 뉴스 카테고리 존재 확인\n",
    "set(news_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['BH', \"Con/Party\", \"North\", \"Admin\", \"Defence/Diplo\", \"Politic\"]\n",
    "pd.DataFrame({\"cate_name\" : category}).to_csv(\"news_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cate_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Con/Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Defence/Diplo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Politic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cate_name\n",
       "0             BH\n",
       "1      Con/Party\n",
       "2          North\n",
       "3          Admin\n",
       "4  Defence/Diplo\n",
       "5        Politic"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = pd.read_csv(\"news_class.csv\", index_col = 0)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(documents):\n",
    "    max_document_length = 1000\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.transform(documents)))\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "    vocabulary  = list(list(zip(*sorted_vocab))[0])\n",
    "    return x, vocabulary, len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output(category):\n",
    "    classes = pd.read_csv(\"news_class.csv\", index_col = 0)\n",
    "    one_hot_vectors = np.eye(len(classes), dtype = int)\n",
    "    class_vectors = {}\n",
    "    y = []\n",
    "    for i, cls in enumerate(list(classes[\"cate_name\"])):\n",
    "        class_vectors[cls] = one_hot_vectors[i]\n",
    "    for c in category:\n",
    "        y.append(class_vectors[c])\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 함수에 대한 설명 : ~~\n",
    "\n",
    "def load_data():\n",
    "    cnn = pd.read_csv(\"news_dataset.csv\", index_col = 0)\n",
    "    contents = cnn[\"content\"]\n",
    "    category = cnn[\"label\"]\n",
    "    \n",
    "    x, vocabulary, vocabulary_size = make_input(contents)\n",
    "    \n",
    "    y = make_output(category)\n",
    "    return x, y, vocabulary, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loading\n",
    "\n",
    "x, y, vocabulary, vocabulary_size = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x.shape[1]\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters= 512\n",
    "drop = 0.5\n",
    "epochs = 50\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a tensor\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=6, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 256)    9347328     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1000, 256, 1) 0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 998, 1, 512)  393728      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 997, 1, 512)  524800      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 996, 1, 512)  655872      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 512)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1536)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            9222        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,930,950\n",
      "Trainable params: 10,930,950\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 292 samples, validate on 73 samples\n",
      "Epoch 1/50\n",
      "292/292 [==============================] - 3s 9ms/step - loss: 0.4491 - acc: 0.8333 - val_loss: 0.4352 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83333, saving model to weights.001-0.8333.hdf5\n",
      "Epoch 2/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.4184 - acc: 0.8333 - val_loss: 0.4187 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3868 - acc: 0.8333 - val_loss: 0.4065 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3642 - acc: 0.8350 - val_loss: 0.3982 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3492 - acc: 0.8453 - val_loss: 0.3935 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3369 - acc: 0.8664 - val_loss: 0.3910 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3227 - acc: 0.8767 - val_loss: 0.3895 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3101 - acc: 0.8893 - val_loss: 0.3886 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.83333 to 0.83562, saving model to weights.008-0.8356.hdf5\n",
      "Epoch 9/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.3013 - acc: 0.8955 - val_loss: 0.3874 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.83562 to 0.84247, saving model to weights.009-0.8425.hdf5\n",
      "Epoch 10/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2908 - acc: 0.8995 - val_loss: 0.3860 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2811 - acc: 0.9047 - val_loss: 0.3845 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.84247 to 0.84475, saving model to weights.011-0.8447.hdf5\n",
      "Epoch 12/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2734 - acc: 0.9081 - val_loss: 0.3831 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.84475 to 0.84932, saving model to weights.012-0.8493.hdf5\n",
      "Epoch 13/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2621 - acc: 0.9081 - val_loss: 0.3817 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2545 - acc: 0.9098 - val_loss: 0.3806 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2495 - acc: 0.9098 - val_loss: 0.3789 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2359 - acc: 0.9167 - val_loss: 0.3780 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84932 to 0.85388, saving model to weights.016-0.8539.hdf5\n",
      "Epoch 17/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2299 - acc: 0.9212 - val_loss: 0.3764 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.85388 to 0.85845, saving model to weights.017-0.8584.hdf5\n",
      "Epoch 18/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2218 - acc: 0.9229 - val_loss: 0.3755 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2145 - acc: 0.9287 - val_loss: 0.3747 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.85845 to 0.86073, saving model to weights.019-0.8607.hdf5\n",
      "Epoch 20/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2069 - acc: 0.9281 - val_loss: 0.3736 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.86073 to 0.86301, saving model to weights.020-0.8630.hdf5\n",
      "Epoch 21/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.2015 - acc: 0.9264 - val_loss: 0.3715 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1896 - acc: 0.9332 - val_loss: 0.3701 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1833 - acc: 0.9384 - val_loss: 0.3691 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.86301 to 0.86530, saving model to weights.023-0.8653.hdf5\n",
      "Epoch 24/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1790 - acc: 0.9361 - val_loss: 0.3686 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1747 - acc: 0.9412 - val_loss: 0.3672 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.86530 to 0.86758, saving model to weights.025-0.8676.hdf5\n",
      "Epoch 26/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1670 - acc: 0.9469 - val_loss: 0.3658 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.86758 to 0.87671, saving model to weights.026-0.8767.hdf5\n",
      "Epoch 27/50\n",
      "292/292 [==============================] - 1s 3ms/step - loss: 0.1588 - acc: 0.9486 - val_loss: 0.3647 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/50\n",
      "292/292 [==============================] - 1s 3ms/step - loss: 0.1548 - acc: 0.9566 - val_loss: 0.3629 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1486 - acc: 0.9578 - val_loss: 0.3616 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 30/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1413 - acc: 0.9635 - val_loss: 0.3606 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1369 - acc: 0.9686 - val_loss: 0.3598 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 32/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1315 - acc: 0.9686 - val_loss: 0.3583 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 33/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1268 - acc: 0.9755 - val_loss: 0.3573 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 34/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1215 - acc: 0.9760 - val_loss: 0.3563 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 35/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1173 - acc: 0.9777 - val_loss: 0.3552 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 36/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1125 - acc: 0.9829 - val_loss: 0.3539 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 37/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1069 - acc: 0.9857 - val_loss: 0.3527 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 38/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.1025 - acc: 0.9880 - val_loss: 0.3518 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 39/50\n",
      "292/292 [==============================] - 1s 3ms/step - loss: 0.0975 - acc: 0.9886 - val_loss: 0.3510 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.87671 to 0.88356, saving model to weights.039-0.8836.hdf5\n",
      "Epoch 40/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0938 - acc: 0.9892 - val_loss: 0.3497 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 41/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0901 - acc: 0.9909 - val_loss: 0.3490 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.88356 to 0.88584, saving model to weights.041-0.8858.hdf5\n",
      "Epoch 42/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0853 - acc: 0.9926 - val_loss: 0.3479 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 43/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0813 - acc: 0.9932 - val_loss: 0.3469 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00043: val_acc did not improve\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 1s 3ms/step - loss: 0.0777 - acc: 0.9937 - val_loss: 0.3457 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 45/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0758 - acc: 0.9937 - val_loss: 0.3451 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 46/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0732 - acc: 0.9943 - val_loss: 0.3442 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 47/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0708 - acc: 0.9960 - val_loss: 0.3431 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 48/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0656 - acc: 0.9960 - val_loss: 0.3422 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 49/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0666 - acc: 0.9960 - val_loss: 0.3411 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 50/50\n",
      "292/292 [==============================] - 1s 2ms/step - loss: 0.0613 - acc: 0.9960 - val_loss: 0.3407 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00050: val_acc did not improve\n"
     ]
    }
   ],
   "source": [
    "## Model Training...\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 10)\n",
    "history_1 = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "                     callbacks=[checkpoint, \n",
    "                                early_stopping, \n",
    "                                TensorBoard(log_dir = \"/tmp/logs/history_1\")], \n",
    "                     validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 809us/step\n",
      "Test score: 0.34068038733038186\n",
      "Test accuracy: 0.8835616568996482\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./history_1_graph.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./history_1_scala.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
